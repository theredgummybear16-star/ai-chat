name: Start AI WebUI

on:
  workflow_dispatch:
  push:
    branches: [ main ]

jobs:
  run-ai:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Maximize build space
        run: |
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc "/usr/local/share/boost" "$AGENT_TOOLSDIRECTORY"
          df -h

      - name: Pull Images with Retries
        run: |
          echo "Pulling Ollama..."
          docker pull ollama/ollama:latest
          
          echo "Pulling Open WebUI (GHCR)..."
          for i in {1..5}; do 
            docker pull ghcr.io/open-webui/open-webui:main && break || (echo "Retry $i..." && sleep 20)
          done

      - name: Start Services
        run: |
          docker compose up -d
          echo "Waiting for services to start on host network..."
          sleep 60
          docker ps

      - name: Pre-load AI Model
        run: |
          echo "Pulling llama3.2:1b inside Ollama container..."
          docker exec ollama ollama pull llama3.2:1b

      - name: Setup Cloudflared Tunnel
        run: |
          wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
          sudo dpkg -i cloudflared-linux-amd64.deb

      - name: Expose and Connect
        run: |
          echo "=========================================================="
          echo "ðŸš€ YOUR AI IS RUNNING ON THE HOST NETWORK!"
          echo "Access Port: 8080"
          echo "Look for the '.trycloudflare.com' link below."
          echo "=========================================================="
          # Open WebUI uses port 8080 by default when in host mode
          cloudflared tunnel --url http://localhost:8080
